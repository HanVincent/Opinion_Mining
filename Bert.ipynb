{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en_US.UTF-8'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F   # 神經網絡模塊中的常用功能 \n",
    "import locale\n",
    "from utils.evaluate import *\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features by Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract pre-computed feature vectors from a PyTorch BERT model.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, text, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "          unique_id: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.unique_id = unique_id\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, input_ids, input_mask, segment_ids, label_ids,):\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        #self.label_mask = label_mask\n",
    "        \n",
    "        \n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf8') as f:\n",
    "            lines, words, labels = [], [], []\n",
    "            for line in f:\n",
    "                contents = line.strip()\n",
    "                word = contents.split('\\t')[0]\n",
    "                label = contents.split('\\t')[-1]\n",
    "\n",
    "                if len(contents) == 0 and words[-1] == '.':\n",
    "                    w = ' '.join([word for word in words if len(word) > 0])\n",
    "                    l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                    lines.append([w, l])\n",
    "                    words = []\n",
    "                    labels = []\n",
    "                    continue\n",
    "                    \n",
    "                words.append(word)\n",
    "                labels.append(label)\n",
    "            return lines\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_train_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"train.txt\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"dev.txt\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"B-DSE\", \"I-DSE\", \"O\", \"[CLS]\", \"[SEP]\"]\n",
    "    \n",
    "    def get_label_map(self):\n",
    "        return {\"O\": 0, \"B-DSE\": 1, \"I-DSE\": 2, \"[CLS]\": 3, \"[SEP]\": 4}\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            unique_id = \"%s-%s\" % (set_type, i)\n",
    "            text, label = line\n",
    "            examples.append(InputExample(unique_id=unique_id, text=text, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "            \n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_length, tokenizer, label_map):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        ### tokenize data\n",
    "        text_list = example.text.split(' ')\n",
    "        label_list = example.label.split(' ')\n",
    "        \n",
    "        tokens, labels = [], []\n",
    "        for i, (w, l) in enumerate(zip(text_list, label_list)):\n",
    "            tk = tokenizer.tokenize(w)\n",
    "            tokens.extend(tk)            \n",
    "            labels.extend([l if m == 0 else \"I-DSE\" for m in range(len(tk))])\n",
    "\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[0 : (max_seq_length - 2)]\n",
    "            labels = labels[0 : (max_seq_length - 2)]\n",
    "\n",
    "        ### insert CLS and SEP\n",
    "        # label_ids append(\"O\") or append(\"[CLS]\") not sure!\n",
    "        ntokens, segment_ids, label_ids = [\"[CLS]\"], [0], [label_map[\"[CLS]\"]]\n",
    "        for i, (tk, l) in enumerate(zip(tokens, labels)):\n",
    "            ntokens.append(tk)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(label_map[l])\n",
    "            \n",
    "        ntokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "        label_ids.append(label_map[\"[SEP]\"])\n",
    "        \n",
    "        ### convert to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        \n",
    "        ### create mask\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        ### padding to max seq len\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            # we don't concerned about it!\n",
    "            label_ids.append(0)\n",
    "        \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        \n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in ntokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"input_type_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                tokens=ntokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                label_ids=label_ids))\n",
    "    return features\n",
    "\n",
    "\n",
    "def freeze_encoder_to(model, to=-1):\n",
    "    if to < 0:\n",
    "        to = len(model.encoder.layer) + to + 1\n",
    "    for idx in range(to):\n",
    "        for param in model.encoder.layer[idx].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    print(\"Encoder freezed to {}\".format(to))\n",
    "    to = len(model.encoder.layer)\n",
    "    for idx in range(idx, to):\n",
    "        for param in model.encoder.layer[idx].parameters():\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        \n",
    "    logger.info(\"device: {} n_gpu: {} distributed training: {}\".format(device, n_gpu, bool(args.local_rank != -1)))\n",
    "\n",
    "    layer_indexes = [int(x) for x in args.layers.split(\",\")]\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "\n",
    "    ###\n",
    "    # examples = read_examples(args.input_file)\n",
    "    processor = NerProcessor(args.input_dir)\n",
    "    \n",
    "    label_list = processor.get_labels()\n",
    "    label_map = processor.get_label_map()\n",
    "    \n",
    "    examples = processor.get_train_examples()\n",
    "\n",
    "    features = convert_examples_to_features(\n",
    "        examples=examples, max_seq_length=args.max_seq_length, tokenizer=tokenizer, label_map=label_map)\n",
    "\n",
    "    model = BertModel.from_pretrained(args.bert_model)\n",
    "    model.to(device)\n",
    "         \n",
    "    if args.isfrozen:\n",
    "        freeze_encoder_to(model)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_input_labels = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_input_labels, all_example_index)\n",
    "    \n",
    "    if args.local_rank == -1:\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "    else:\n",
    "        eval_sampler = DistributedSampler(eval_data)\n",
    "        \n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    ### Linear layer\n",
    "    linear_model = torch.nn.Linear(768, len(label_list))\n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, linear_model.parameters()), \n",
    "                              lr=args.learning_rate, momentum=args.momentum)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    linear_model.to(device)\n",
    "    ###\n",
    "    \n",
    "    for input_ids, input_mask, input_labels, example_indices in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        input_labels = input_labels.to(device)\n",
    "\n",
    "        all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "\n",
    "        if args.mode == \"last\":\n",
    "            all_encoder_layers = all_encoder_layers[-1]\n",
    "#         elif args.mode == \"weighted\":\n",
    "#             all_encoder_layers = torch.stack([a * b for a, b in zip(all_encoder_layers, self.bert_weights)])\n",
    "#             return self.bert_gamma * torch.sum(all_encoder_layers, dim=0)\n",
    "\n",
    "        y_pred = linear_model(all_encoder_layers)\n",
    "        y_pred = F.log_softmax(y_pred, dim=2)\n",
    "        y_pred = y_pred.view(-1, len(label_list))\n",
    "\n",
    "        tags = input_labels.view(-1)\n",
    "\n",
    "        loss = loss_function(y_pred, tags)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      \n",
    "    ###################### TEST DEV ###\n",
    "    eval_examples = processor.get_dev_examples()\n",
    "    eval_features = convert_examples_to_features(\n",
    "        examples=eval_examples, max_seq_length=args.max_seq_length, tokenizer=tokenizer, label_map=label_map)\n",
    "\n",
    "\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.batch_size)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in eval_features], dtype=torch.long)\n",
    "    \n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    if args.local_rank == -1:\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "    else:\n",
    "        eval_sampler = DistributedSampler(eval_data)\n",
    "\n",
    "    batch_size = len(all_input_ids)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # should be only once\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "            if args.mode == \"last\":\n",
    "                all_encoder_layers = all_encoder_layers[-1]\n",
    "\n",
    "            y_pred = linear_model(all_encoder_layers)\n",
    "            y_pred = F.log_softmax(y_pred, dim=2)\n",
    "            # y_pred = y_pred.view(-1, len(label_list))\n",
    "            y_predicts = torch.max(y_pred, 2)[1].view([batch_size, -1])\n",
    "            print(y_predicts.shape)\n",
    "            print(y_predicts)\n",
    "            y_predicts = [y_[:len(y_trues[i])] for i, y_ in enumerate(y_predicts)]\n",
    "\n",
    "            result = evaluate(y_predicts, y_trues, label_map)\n",
    "\n",
    "\n",
    "    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/28/2018 13:19:12 - INFO - __main__ -   device: cuda n_gpu: 2 distributed training: False\n",
      "11/28/2018 13:19:13 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   unique_id: train-0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   tokens: [CLS] on monday 28 january , the us national security council convened for an uncomfortable meeting ; the main issue to be debated was known as early as the week before , following an unprecedented split that had emerged in the us administration . [SEP]\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_ids: 101 2006 6928 2654 2254 1010 1996 2149 2120 3036 2473 19596 2005 2019 8796 3116 1025 1996 2364 3277 2000 2022 15268 2001 2124 2004 2220 2004 1996 2733 2077 1010 2206 2019 15741 3975 2008 2018 6003 1999 1996 2149 3447 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   label_ids: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   unique_id: train-1\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   tokens: [CLS] these were some of the leaders called to express their opinions : secretary of defense donald rum ##sf ##eld , vice president dick cheney , john ash ##croft - l ##rb - general prosecutor and chief of the justice department - rr ##b - , cia director george ten ##et , general richard b . myers , chairman of the joint chiefs of staff , and others . [SEP]\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_ids: 101 2122 2020 2070 1997 1996 4177 2170 2000 4671 2037 10740 1024 3187 1997 3639 6221 19379 22747 14273 1010 3580 2343 5980 23745 1010 2198 6683 14716 1011 1048 15185 1011 2236 12478 1998 2708 1997 1996 3425 2533 1011 25269 2497 1011 1010 9915 2472 2577 2702 3388 1010 2236 2957 1038 1012 13854 1010 3472 1997 1996 4101 9058 1997 3095 1010 1998 2500 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   label_ids: 3 0 0 0 0 0 0 0 1 2 2 2 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 2 0 2 2 2 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   unique_id: train-2\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   tokens: [CLS] president george w . bush and secretary of state colin powell also participated in this business . [SEP]\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_ids: 101 2343 2577 1059 1012 5747 1998 3187 1997 2110 6972 8997 2036 4194 1999 2023 2449 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   label_ids: 3 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   unique_id: train-3\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   tokens: [CLS] the issue at stake was apparently simple : the status of the detainees , members of al - q ##a ` ida and taliban fighters captured by us troops in afghanistan in recent months and recently transferred to a detention base at guantanamo bay . [SEP]\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_ids: 101 1996 3277 2012 8406 2001 4593 3722 1024 1996 3570 1997 1996 26485 1010 2372 1997 2632 1011 1053 2050 1036 16096 1998 16597 7299 4110 2011 2149 3629 1999 7041 1999 3522 2706 1998 3728 4015 2000 1037 12345 2918 2012 23094 3016 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   label_ids: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   unique_id: train-4\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   tokens: [CLS] eventually , president bush agreed to rec ##ons ##ider some of the decisions he had announced a short time previously . [SEP]\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_ids: 101 2776 1010 2343 5747 3530 2000 28667 5644 18688 2070 1997 1996 6567 2002 2018 2623 1037 2460 2051 3130 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:14 - INFO - __main__ -   label_ids: 3 0 0 0 0 1 1 2 2 2 0 0 0 1 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:19:22 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "11/28/2018 13:19:22 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpf6mu_5mq\n",
      "11/28/2018 13:19:26 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder freezed to 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/28/2018 13:21:04 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   unique_id: dev-0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   tokens: [CLS] international condemnation of mug ##abe ' s win mounted yesterday with us president george bush and british foreign secretary jack straw delivering further criticism . [SEP]\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_ids: 101 2248 26248 1997 14757 16336 1005 1055 2663 5614 7483 2007 2149 2343 2577 5747 1998 2329 3097 3187 2990 13137 12771 2582 6256 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   label_ids: 3 0 1 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   unique_id: dev-1\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   tokens: [CLS] bush said : ` ` we are dealing with our friends to figure out how to deal with this flawed election . ' ' straw said britain did ` ` not recognize the result or its legitimacy ' ' . [SEP]\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_ids: 101 5747 2056 1024 1036 1036 2057 2024 7149 2007 2256 2814 2000 3275 2041 2129 2000 3066 2007 2023 25077 2602 1012 1005 1005 13137 2056 3725 2106 1036 1036 2025 6807 1996 2765 2030 2049 22568 1005 1005 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   label_ids: 3 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 2 2 2 2 0 0 0 0 0 0 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   unique_id: dev-2\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   tokens: [CLS] he hinted at a tough ##er policy against mug ##abe , saying that the european union would review sanctions at an upcoming summit meeting in barcelona , spain . [SEP]\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_ids: 101 2002 21795 2012 1037 7823 2121 3343 2114 14757 16336 1010 3038 2008 1996 2647 2586 2052 3319 17147 2012 2019 9046 6465 3116 1999 7623 1010 3577 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   label_ids: 3 0 1 2 0 1 2 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   unique_id: dev-3\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   tokens: [CLS] para ##tro ##opers return chavez to power once again , events in venezuela have drawn the close attention of the world ' s media . [SEP]\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_ids: 101 11498 13181 27342 2709 16860 2000 2373 2320 2153 1010 2824 1999 8326 2031 4567 1996 2485 3086 1997 1996 2088 1005 1055 2865 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   label_ids: 3 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 0 0 0 0 2 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   *** Example ***\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   unique_id: dev-4\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   tokens: [CLS] hardly had observers even time to get used to the news that hugo chavez had lost his presidential post , and then he returned to power . [SEP]\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_ids: 101 6684 2018 14009 2130 2051 2000 2131 2109 2000 1996 2739 2008 9395 16860 2018 2439 2010 4883 2695 1010 1998 2059 2002 2513 2000 2373 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:04 - INFO - __main__ -   label_ids: 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/28/2018 13:21:05 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/28/2018 13:21:05 - INFO - __main__ -     Num examples = 1218\n",
      "11/28/2018 13:21:05 - INFO - __main__ -     Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1218, 128])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_trues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-a9b78b423042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-c5af60117441>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0my_predicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-c5af60117441>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0my_predicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_trues' is not defined"
     ]
    }
   ],
   "source": [
    "class Args(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_dir = './dse/'\n",
    "        self.output_file = '../pytorch-pretrained-BERT/examples/output'\n",
    "        self.bert_model = 'bert-base-uncased'\n",
    "        self.mode = 'last'\n",
    "        self.layers = \"-1,-2,-3,-4\"\n",
    "        self.max_seq_length = 128\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 1e-4\n",
    "        self.momentum = 0.7\n",
    "        self.isfrozen = True\n",
    "        self.local_rank = -1\n",
    "        self.no_cuda = False\n",
    "        \n",
    "main(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ## Required parameters\n",
    "    parser.add_argument(\"--input_file\", default=None, type=str, required=True)\n",
    "    parser.add_argument(\"--output_file\", default=None, type=str, required=True)\n",
    "    parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "                             \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--layers\", default=\"-1,-2,-3,-4\", type=str)\n",
    "    parser.add_argument(\"--mode\", default=\"last\", type=str)\n",
    "    parser.add_argument(\"--max_seq_length\", default=128, type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"\n",
    "                            \"than this will be truncated, and sequences shorter than this will be padded.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size for predictions.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-4, type=float, help=\"Learning rate for gradient.\")\n",
    "    parser.add_argument(\"--momentum\", default=0.7, type=float)\n",
    "    parser.add_argument(\"--isfrozen\", default=True, type=bool)\n",
    "    parser.add_argument(\"--local_rank\",\n",
    "                        type=int,\n",
    "                        default=-1,\n",
    "                        help = \"local_rank for distributed training on gpus\")\n",
    "    parser.add_argument(\"--no_cuda\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for sent, tags in zip(train_sentences, train_tags):\n",
    "\n",
    "        assert len(sent) == len(tags)\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(sent)\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "        # Predict hidden states features for each layer\n",
    "        encoded_layers, _ = model(tokens_tensor)\n",
    "        output_layer = encoded_layers[-1]\n",
    "\n",
    "        y_pred = linear_model(output_layer)\n",
    "        y_pred = F.log_softmax(y_pred, dim=2)\n",
    "\n",
    "        y_pred = y_pred.view(-1, len(tag_to_index))\n",
    "\n",
    "        tags = torch.tensor([tags]).view(-1)\n",
    "\n",
    "        loss = loss_function(y_pred, tags)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
