{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features by Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract pre-computed feature vectors from a PyTorch BERT model.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # 神經網絡模塊中的常用功能 \n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from utils.evaluate import *\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_PARAENTHESIS = ['-LRB-', '-LSB-', '-LCB-']\n",
    "RIGHT_PARAENTHESIS = ['-RRB-', '-RSB-', '-RCB-']\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, text, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "          unique_id: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.unique_id = unique_id\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, input_ids, input_mask, input_type_ids, label_ids):\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "        self.label_ids = label_ids\n",
    "        #self.label_mask = label_mask\n",
    "        \n",
    "        \n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_label_map(self):\n",
    "        \"\"\"Gets the mapping of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf8') as f:\n",
    "            lines, words, labels = [], [], []\n",
    "\n",
    "            for line in f:\n",
    "                contents = line.strip()\n",
    "                word = contents.split('\\t')[0]\n",
    "                label = contents.split('\\t')[-1]\n",
    "\n",
    "\n",
    "                if len(contents) == 0: # and words[-1] == '.':\n",
    "                    w = ' '.join([word for word in words if len(word) > 0])\n",
    "                    l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                    lines.append([w, l])\n",
    "                    words = []\n",
    "                    labels = []\n",
    "                    continue\n",
    "\n",
    "                if word in LEFT_PARAENTHESIS: word = '('\n",
    "                elif word in RIGHT_PARAENTHESIS: word = ')'\n",
    "                    \n",
    "                words.append(word)\n",
    "                labels.append(label)\n",
    "\n",
    "        return lines\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_train_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"train.txt\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"dev.txt\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return {\"O\": 0, \"B-DSE\": 1, \"I-DSE\": 2}\n",
    "        # return {\"O\": 0, \"B-DSE\": 1, \"I-DSE\": 2, \"[CLS]\": 3, \"[SEP]\": 4}\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            unique_id = \"%s-%s\" % (set_type, i)\n",
    "            text, label = line\n",
    "            examples.append(InputExample(unique_id=unique_id, text=text, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "            \n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_length, tokenizer, label_map):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        ### tokenize data\n",
    "        text_list = example.text.split(' ')\n",
    "        label_list = example.label.split(' ')\n",
    "        \n",
    "        assert len(text_list) == len(label_list)\n",
    "        \n",
    "        tokens, labels = [], []\n",
    "        for i, (w, l) in enumerate(zip(text_list, label_list)):\n",
    "            tk = tokenizer.tokenize(w)\n",
    "            tokens.extend(tk)\n",
    "            for m in range(len(tk)):\n",
    "                if m == 0:         labels.append(l)\n",
    "                elif l == \"B-DSE\": labels.append(\"I-DSE\")\n",
    "                else:              labels.append(\"O\")\n",
    "\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[0 : (max_seq_length - 2)]\n",
    "            labels = labels[0 : (max_seq_length - 2)]\n",
    "\n",
    "        ### insert CLS and SEP\n",
    "        # label_ids append(\"O\") or append(\"[CLS]\") not sure!\n",
    "        ntokens, input_type_ids, label_ids = [\"[CLS]\"], [0], [label_map[\"O\"]]\n",
    "        for i, (tk, l) in enumerate(zip(tokens, labels)):\n",
    "            ntokens.append(tk)\n",
    "            input_type_ids.append(0)\n",
    "            label_ids.append(label_map[l])\n",
    "            \n",
    "        ntokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(0)\n",
    "        # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "        label_ids.append(label_map[\"O\"])\n",
    "        \n",
    "        ### convert to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        \n",
    "        ### create mask\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        ### padding to max seq len\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            input_type_ids.append(0)\n",
    "            # we don't concerned about it!\n",
    "            label_ids.append(0)\n",
    "        \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(input_type_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        \n",
    "        if ex_index < 2:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in ntokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            logger.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                tokens=ntokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids,\n",
    "                label_ids=label_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_features_to_dataloader(features, local_rank, batch_size):\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_input_labels = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_input_labels, all_example_index)\n",
    "    \n",
    "    if local_rank == -1: sampler = SequentialSampler(dataset)\n",
    "    else:                sampler = DistributedSampler(dataset)\n",
    "        \n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "class BertTagger(nn.Module):\n",
    " \n",
    "    def __init__(self, bert_model, hidden_dim, label_map, loss_function,\n",
    "                 is_frozen=True, mode=\"last\"):\n",
    "        super(BertTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.label_map = label_map\n",
    "        self.loss_function = loss_function\n",
    "        self.tagset_size = len(self.label_map)\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(bert_model)\n",
    "        \n",
    "        if is_frozen: self.bert_model.eval()\n",
    "            \n",
    "        self.linear_model = torch.nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "\n",
    "        \n",
    "    def _forward_alg(self, input_ids, input_mask):\n",
    "        batch_size, max_seq_len = input_ids.shape\n",
    "        \n",
    "        all_encoder_layers, _ = self.bert_model(input_ids, \n",
    "                                                token_type_ids=None, \n",
    "                                                attention_mask=input_mask)\n",
    "        \n",
    "        if self.mode == \"last\":\n",
    "            all_encoder_layers = all_encoder_layers[-1]\n",
    "#         elif args.mode == \"weighted\":\n",
    "#             all_encoder_layers = torch.stack([a * b for a, b in zip(all_encoder_layers, self.bert_weights)])\n",
    "#             return self.bert_gamma * torch.sum(all_encoder_layers, dim=0)\n",
    "        \n",
    "        y_pred = self.linear_model(all_encoder_layers)\n",
    "        y_pred = F.log_softmax(y_pred, dim=2)\n",
    "\n",
    "        ### not sure mask\n",
    "#         y_ = torch.mul(tag_scores, mask.unsqueeze(-1).expand([batch_size, seq_len, self.tagset_size]))\n",
    "        y_pred = y_pred.view(-1, self.tagset_size)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, input_labels):\n",
    "        y_pred = self._forward_alg(input_ids, input_mask)\n",
    "        y_true = input_labels.view(-1)\n",
    "\n",
    "        loss = self.loss_function(y_pred, y_true)\n",
    "\n",
    "        return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        \n",
    "    logger.info(\"device: {} n_gpu: {} distributed training: {}\".format(device, n_gpu, bool(args.local_rank != -1)))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "\n",
    "    processor = NerProcessor(args.input_dir)\n",
    "    label_map = processor.get_label_map()\n",
    "    examples = processor.get_train_examples()\n",
    "\n",
    "    features = convert_examples_to_features(examples=examples, \n",
    "                                            max_seq_length=args.max_seq_length, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            label_map=label_map)\n",
    "    \n",
    "    dataloader = convert_features_to_dataloader(features=features, \n",
    "                                                local_rank=args.local_rank, \n",
    "                                                batch_size=args.batch_size)\n",
    "\n",
    "    \n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    \n",
    "    model = BertTagger(args.bert_model, hidden_dim=768, \n",
    "                       label_map=label_map, mode=\"last\",\n",
    "                      loss_function=loss_function)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                              lr=args.learning_rate, momentum=args.momentum)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, \n",
    "                                                          device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    ######### TRAIN\n",
    "#     for epoch in range(args.epochs):\n",
    "#         print(\"Epoch:\", epoch)\n",
    "        \n",
    "#         for input_ids, input_mask, input_labels, example_indices in dataloader:\n",
    "#             input_ids = input_ids.to(device)\n",
    "#             input_mask = input_mask.to(device)\n",
    "#             input_labels = input_labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             _, loss = model(input_ids, input_mask, input_labels)\n",
    "            \n",
    "#             if n_gpu > 1:\n",
    "#                 loss = loss.mean() # mean() to average on multi-gpu.\n",
    "#             if args.fp16 and args.loss_scale != 1.0:\n",
    "#                 # rescale loss for fp16 training\n",
    "#                 # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "#                 loss = loss * args.loss_scale\n",
    "#             if args.gradient_accumulation_steps > 1:\n",
    "#                 loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     torch.save(model.state_dict(), os.path.join(args.output_dir, \"store.model\"))\n",
    "    \n",
    "    \n",
    "    ######### TEST\n",
    "    model.load_state_dict(torch.load(os.path.join(args.output_dir, \"store.model\")))\n",
    "    \n",
    "    examples = processor.get_test_examples()\n",
    "    \n",
    "    features = convert_examples_to_features(examples=examples, \n",
    "                                             max_seq_length=args.max_seq_length, \n",
    "                                             tokenizer=tokenizer, \n",
    "                                             label_map=label_map)\n",
    "\n",
    "    dataloader = convert_features_to_dataloader(features=features, \n",
    "                                                 local_rank=args.local_rank, \n",
    "                                                 batch_size=args.batch_size)\n",
    "\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(examples))\n",
    "    logger.info(\"  Batch size = %d\", args.batch_size)\n",
    "\n",
    "    y_preds_tk_basis, y_trues_tk_basis = [], []\n",
    "    y_preds, y_trues = [], []\n",
    "   \n",
    "    for input_ids, input_mask, input_labels, example_indices in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        input_labels = input_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred, loss = model(input_ids, input_mask, input_labels)\n",
    "\n",
    "#             y_pred = torch.max(y_pred, 2)[1].view([len(input_ids), -1])\n",
    "            y_pred = torch.max(y_pred, 1)[1].view([len(input_ids), -1])\n",
    "\n",
    "            ### 用 * mask \n",
    "            # y_pred = torch.mul(y_pred, input_mask) # [batch, max_seq_len]\n",
    "            # y_true = torch.mul(input_labels, input_mask)\n",
    "            y_pred = [y_[ : sum(input_mask[i])] for i, y_ in enumerate(y_pred)]\n",
    "            y_true = [y[ : sum(input_mask[i])] for i, y in enumerate(input_labels)]\n",
    "          \n",
    "            y_preds.extend(y_pred)\n",
    "            y_trues.extend(y_true)\n",
    "            y_preds_tk_basis.extend([el for y_ in y_pred for el in y_])\n",
    "            y_trues_tk_basis.extend([el for y_ in y_true for el in y_])\n",
    "            \n",
    "\n",
    "    result = token_basis_evaluate(y_preds_tk_basis, y_trues_tk_basis, label_map, output_dict=False)\n",
    "    print(result)\n",
    "\n",
    "    result = overlap_evaluate(y_preds, y_trues, label_map)\n",
    "    print(result)    \n",
    "\n",
    "    import pickle\n",
    "    with open(os.path.join(args.output_dir, \"results.txt\"), \"wb\") as file:\n",
    "        pickle.dump([y_preds, y_trues, y_preds_tk_basis, y_trues_tk_basis], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2018 05:58:56 - INFO - __main__ -   device: cuda n_gpu: 2 distributed training: False\n",
      "12/04/2018 05:58:57 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/nlplab/vincent/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   *** Example ***\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   unique_id: train-0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   tokens: [CLS] on monday 28 january , the us national security council convened for an uncomfortable meeting ; the main issue to be debated was known as early as the week before , following an unprecedented split that had emerged in the us administration . [SEP]\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   input_ids: 101 2006 6928 2654 2254 1010 1996 2149 2120 3036 2473 19596 2005 2019 8796 3116 1025 1996 2364 3277 2000 2022 15268 2001 2124 2004 2220 2004 1996 2733 2077 1010 2206 2019 15741 3975 2008 2018 6003 1999 1996 2149 3447 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   label_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   *** Example ***\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   unique_id: train-1\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   tokens: [CLS] these were some of the leaders called to express their opinions : secretary of defense donald rum ##sf ##eld , vice president dick cheney , john ash ##croft ( general prosecutor and chief of the justice department ) , cia director george ten ##et , general richard b . myers , chairman of the joint chiefs of staff , and others . [SEP]\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   input_ids: 101 2122 2020 2070 1997 1996 4177 2170 2000 4671 2037 10740 1024 3187 1997 3639 6221 19379 22747 14273 1010 3580 2343 5980 23745 1010 2198 6683 14716 1006 2236 12478 1998 2708 1997 1996 3425 2533 1007 1010 9915 2472 2577 2702 3388 1010 2236 2957 1038 1012 13854 1010 3472 1997 1996 4101 9058 1997 3095 1010 1998 2500 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:58:58 - INFO - __main__ -   label_ids: 0 0 0 0 0 0 0 0 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:04 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/nlplab/vincent/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "12/04/2018 05:59:04 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/nlplab/vincent/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpz9hdnyb1\n",
      "12/04/2018 05:59:06 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   *** Example ***\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   unique_id: test-0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   tokens: [CLS] under the third geneva convention , prisoners of war may only be tried in the same courts and according to the same rules , as soldiers of the country that is holding the prisoners . [SEP]\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   input_ids: 101 2104 1996 2353 9810 4680 1010 5895 1997 2162 2089 2069 2022 2699 1999 1996 2168 5434 1998 2429 2000 1996 2168 3513 1010 2004 3548 1997 1996 2406 2008 2003 3173 1996 5895 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   label_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   *** Example ***\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   unique_id: test-1\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   tokens: [CLS] that means the al qaeda suspects could not be tried in the special military tribunal ##s whose rules are currently being worked out , but only by regular us military courts using the uniform code of military justice . [SEP]\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   input_ids: 101 2008 2965 1996 2632 18659 13172 2071 2025 2022 2699 1999 1996 2569 2510 12152 2015 3005 3513 2024 2747 2108 2499 2041 1010 2021 2069 2011 3180 2149 2510 5434 2478 1996 6375 3642 1997 2510 3425 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:10 - INFO - __main__ -   label_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2018 05:59:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "12/04/2018 05:59:11 - INFO - __main__ -     Num examples = 1450\n",
      "12/04/2018 05:59:11 - INFO - __main__ -     Batch size = 64\n",
      "/usr/local/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:58: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.96      1.00      0.98     43596\n",
      "       B-DSE       0.56      0.26      0.35      1331\n",
      "       I-DSE       0.43      0.04      0.07      1226\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     46153\n",
      "   macro avg       0.65      0.43      0.47     46153\n",
      "weighted avg       0.93      0.95      0.93     46153\n",
      "\n",
      "{'binary': {'precision': 0.82, 'recall': 0.3681116825863336, 'f1': 0.5081198748314801}, 'proportional': {'precision': 0.8176190476190475, 'recall': 0.28845036912634225, 'f1': 0.426451563563541}}\n"
     ]
    }
   ],
   "source": [
    "class Args(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_dir = './dse/'\n",
    "        self.output_dir = '.'\n",
    "        self.bert_model = 'bert-base-uncased'\n",
    "        self.mode = 'last'\n",
    "        self.max_seq_length = 128\n",
    "        self.epochs = 200\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 1e-4\n",
    "        self.momentum = 0.7\n",
    "        self.is_frozen = True\n",
    "        self.local_rank = -1\n",
    "        self.no_cuda = False\n",
    "        self.fp16 = False\n",
    "        self.loss_scale = 128.\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        \n",
    "main(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     ## Required parameters\n",
    "#     parser.add_argument(\"--input_dir\", default=None, type=str, required=True)\n",
    "#     parser.add_argument(\"--output_dir\", default=None, type=str, required=True)\n",
    "#     parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "#                         help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "#                              \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "\n",
    "#     ## Other parameters\n",
    "#     parser.add_argument(\"--mode\", default=\"last\", type=str)\n",
    "#     parser.add_argument(\"--max_seq_length\", default=128, type=int,\n",
    "#                         help=\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"\n",
    "#                             \"than this will be truncated, and sequences shorter than this will be padded.\")\n",
    "#     parser.add_argument(\"--epochs\", default=200, type=int, help=\"Number of epoch.\")\n",
    "#     parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size for predictions.\")\n",
    "#     parser.add_argument(\"--learning_rate\", default=1e-4, type=float, help=\"Learning rate for gradient.\")\n",
    "#     parser.add_argument(\"--momentum\", default=0.7, type=float)\n",
    "#     parser.add_argument(\"--is_frozen\", default=True, type=bool)\n",
    "#     parser.add_argument(\"--local_rank\",\n",
    "#                         type=int,\n",
    "#                         default=-1,\n",
    "#                         help = \"local_rank for distributed training on gpus\")\n",
    "#     parser.add_argument(\"--no_cuda\",\n",
    "#                         default=False,\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether not to use CUDA when available\")\n",
    "#     parser.add_argument('--fp16',\n",
    "#                         default=False,\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "#     parser.add_argument('--loss_scale',\n",
    "#                         type=float, default=128,\n",
    "#                         help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "# parser.add_argument('--gradient_accumulation_steps',\n",
    "#                         type=int,\n",
    "#                         default=1,\n",
    "#                         help=\"Number of updates steps to accumualte before performing a backward/update pass.\")          \n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
