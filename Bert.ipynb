{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features by Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract pre-computed feature vectors from a PyTorch BERT model.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # 神經網絡模塊中的常用功能 \n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from utils.evaluate import *\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, text, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "          unique_id: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.unique_id = unique_id\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, input_ids, input_mask, input_type_ids, label_ids):\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "        self.label_ids = label_ids\n",
    "        #self.label_mask = label_mask\n",
    "        \n",
    "        \n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_label_map(self):\n",
    "        \"\"\"Gets the mapping of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf8') as f:\n",
    "            lines, words, labels = [], [], []\n",
    "            for line in f:\n",
    "                contents = line.strip()\n",
    "                word = contents.split('\\t')[0]\n",
    "                label = contents.split('\\t')[-1]\n",
    "\n",
    "                if len(contents) == 0 and words[-1] == '.':\n",
    "                    w = ' '.join([word for word in words if len(word) > 0])\n",
    "                    l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                    lines.append([w, l])\n",
    "                    words = []\n",
    "                    labels = []\n",
    "                    continue\n",
    "                    \n",
    "                words.append(word)\n",
    "                labels.append(label)\n",
    "            return lines\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_train_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"train.txt\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"dev.txt\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return {\"O\": 0, \"B-DSE\": 1, \"I-DSE\": 2}\n",
    "        # return {\"O\": 0, \"B-DSE\": 1, \"I-DSE\": 2, \"[CLS]\": 3, \"[SEP]\": 4}\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            unique_id = \"%s-%s\" % (set_type, i)\n",
    "            text, label = line\n",
    "            examples.append(InputExample(unique_id=unique_id, text=text, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "            \n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_length, tokenizer, label_map):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        ### tokenize data\n",
    "        text_list = example.text.split(' ')\n",
    "        label_list = example.label.split(' ')\n",
    "        \n",
    "        tokens, labels = [], []\n",
    "        for i, (w, l) in enumerate(zip(text_list, label_list)):\n",
    "            tk = tokenizer.tokenize(w)\n",
    "            tokens.extend(tk)            \n",
    "            labels.extend([l if m == 0 else \"I-DSE\" for m in range(len(tk))])\n",
    "\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[0 : (max_seq_length - 2)]\n",
    "            labels = labels[0 : (max_seq_length - 2)]\n",
    "\n",
    "        ### insert CLS and SEP\n",
    "        # label_ids append(\"O\") or append(\"[CLS]\") not sure!\n",
    "        ntokens, input_type_ids, label_ids = [\"[CLS]\"], [0], [label_map[\"O\"]]\n",
    "        for i, (tk, l) in enumerate(zip(tokens, labels)):\n",
    "            ntokens.append(tk)\n",
    "            input_type_ids.append(0)\n",
    "            label_ids.append(label_map[l])\n",
    "            \n",
    "        ntokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(0)\n",
    "        # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "        label_ids.append(label_map[\"O\"])\n",
    "        \n",
    "        ### convert to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        \n",
    "        ### create mask\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        ### padding to max seq len\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            input_type_ids.append(0)\n",
    "            # we don't concerned about it!\n",
    "            label_ids.append(0)\n",
    "        \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(input_type_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        \n",
    "        if ex_index < 2:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in ntokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            logger.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                tokens=ntokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids,\n",
    "                label_ids=label_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_features_to_dataloader(features, local_rank, batch_size):\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_input_labels = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_input_labels, all_example_index)\n",
    "    \n",
    "    if local_rank == -1: sampler = SequentialSampler(dataset)\n",
    "    else:                sampler = DistributedSampler(dataset)\n",
    "        \n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "class BertTagger(nn.Module):\n",
    " \n",
    "    def __init__(self, bert_model, hidden_dim, label_map, loss_function,\n",
    "                 is_frozen=True, mode=\"last\"):\n",
    "        super(BertTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.label_map = label_map\n",
    "        self.loss_function = loss_function\n",
    "        self.tagset_size = len(self.label_map)\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(bert_model)\n",
    "        \n",
    "        if is_frozen: self.bert_model.eval()\n",
    "            \n",
    "        self.linear_model = torch.nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "\n",
    "        \n",
    "    def _forward_alg(self, input_ids, input_mask):\n",
    "        batch_size, max_seq_len = input_ids.shape\n",
    "        \n",
    "        all_encoder_layers, _ = self.bert_model(input_ids, \n",
    "                                                token_type_ids=None, \n",
    "                                                attention_mask=input_mask)\n",
    "        \n",
    "        if self.mode == \"last\":\n",
    "            all_encoder_layers = all_encoder_layers[-1]\n",
    "#         elif args.mode == \"weighted\":\n",
    "#             all_encoder_layers = torch.stack([a * b for a, b in zip(all_encoder_layers, self.bert_weights)])\n",
    "#             return self.bert_gamma * torch.sum(all_encoder_layers, dim=0)\n",
    "        \n",
    "        y_pred = self.linear_model(all_encoder_layers)\n",
    "        y_pred = F.log_softmax(y_pred, dim=2)\n",
    "\n",
    "        ### not sure mask\n",
    "#         y_ = torch.mul(tag_scores, mask.unsqueeze(-1).expand([batch_size, seq_len, self.tagset_size]))\n",
    "        y_pred = y_pred.view(-1, self.tagset_size)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, input_labels):\n",
    "        y_pred = self._forward_alg(input_ids, input_mask)\n",
    "        y_true = input_labels.view(-1)\n",
    "\n",
    "        loss = self.loss_function(y_pred, y_true)\n",
    "\n",
    "        return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        \n",
    "    logger.info(\"device: {} n_gpu: {} distributed training: {}\".format(device, n_gpu, bool(args.local_rank != -1)))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "\n",
    "    processor = NerProcessor(args.input_dir)\n",
    "    label_map = processor.get_label_map()\n",
    "    examples = processor.get_train_examples()\n",
    "\n",
    "    features = convert_examples_to_features(examples=examples, \n",
    "                                            max_seq_length=args.max_seq_length, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            label_map=label_map)\n",
    "    \n",
    "    dataloader = convert_features_to_dataloader(features=features, \n",
    "                                                local_rank=args.local_rank, \n",
    "                                                batch_size=args.batch_size)\n",
    "\n",
    "    \n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    \n",
    "    model = BertTagger(args.bert_model, hidden_dim=768, \n",
    "                       label_map=label_map, mode=\"last\",\n",
    "                      loss_function=loss_function)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                              lr=args.learning_rate, momentum=args.momentum)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, \n",
    "                                                          device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    \n",
    "    ######### TRAIN\n",
    "    for epoch in range(args.epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        \n",
    "        for input_ids, input_mask, input_labels, example_indices in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            input_labels = input_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            _, loss = model(input_ids, input_mask, input_labels)\n",
    "            \n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args.fp16 and args.loss_scale != 1.0:\n",
    "                # rescale loss for fp16 training\n",
    "                # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "                loss = loss * args.loss_scale\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(args.output_dir, \"model\"))\n",
    "    \n",
    "    \n",
    "    ######### TEST\n",
    "    examples = processor.get_dev_examples()\n",
    "    features = convert_examples_to_features(examples=examples, \n",
    "                                             max_seq_length=args.max_seq_length, \n",
    "                                             tokenizer=tokenizer, \n",
    "                                             label_map=label_map)\n",
    "\n",
    "    batch_size = len(examples)\n",
    "    dataloader = convert_features_to_dataloader(features=features, \n",
    "                                                 local_rank=args.local_rank, \n",
    "                                                 batch_size=batch_size)\n",
    "\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(examples))\n",
    "    logger.info(\"  Batch size = %d\", batch_size)\n",
    "\n",
    "   \n",
    "    # should be only once\n",
    "    for input_ids, input_mask, input_labels, example_indices in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        input_labels = input_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred, loss = model(input_ids, input_mask, input_labels)\n",
    "\n",
    "#             y_predicts = torch.max(y_pred, 2)[1].view([batch_size, -1])\n",
    "            y_predicts = torch.max(y_pred, 1)[1].view([batch_size, -1])\n",
    "\n",
    "            ### 先暫時用 * mask \n",
    "            # y_predicts = [y_[:len(y_trues[i])] for i, y_ in enumerate(y_predicts)]\n",
    "\n",
    "            y_predicts = torch.mul(y_predicts, input_mask)\n",
    "            y_trues = torch.mul(input_labels, input_mask)\n",
    "\n",
    "            result = evaluate(y_predicts, y_trues, label_map)\n",
    "\n",
    "\n",
    "    with open(os.path.join(args.output_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(result)\n",
    "        writer.write(json.dumps(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/30/2018 06:31:37 - INFO - __main__ -   device: cuda n_gpu: 2 distributed training: False\n",
      "11/30/2018 06:31:38 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   *** Example ***\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   unique_id: train-0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   tokens: [CLS] on monday 28 january , the us national security council convened for an uncomfortable meeting ; the main issue to be debated was known as early as the week before , following an unprecedented split that had emerged in the us administration . [SEP]\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   input_ids: 101 2006 6928 2654 2254 1010 1996 2149 2120 3036 2473 19596 2005 2019 8796 3116 1025 1996 2364 3277 2000 2022 15268 2001 2124 2004 2220 2004 1996 2733 2077 1010 2206 2019 15741 3975 2008 2018 6003 1999 1996 2149 3447 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   label_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   *** Example ***\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   unique_id: train-1\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   tokens: [CLS] these were some of the leaders called to express their opinions : secretary of defense donald rum ##sf ##eld , vice president dick cheney , john ash ##croft - l ##rb - general prosecutor and chief of the justice department - rr ##b - , cia director george ten ##et , general richard b . myers , chairman of the joint chiefs of staff , and others . [SEP]\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   input_ids: 101 2122 2020 2070 1997 1996 4177 2170 2000 4671 2037 10740 1024 3187 1997 3639 6221 19379 22747 14273 1010 3580 2343 5980 23745 1010 2198 6683 14716 1011 1048 15185 1011 2236 12478 1998 2708 1997 1996 3425 2533 1011 25269 2497 1011 1010 9915 2472 2577 2702 3388 1010 2236 2957 1038 1012 13854 1010 3472 1997 1996 4101 9058 1997 3095 1010 1998 2500 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:38 - INFO - __main__ -   label_ids: 0 0 0 0 0 0 0 0 1 2 2 2 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 2 0 2 2 2 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:31:46 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "11/30/2018 06:31:46 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpee85xlpl\n",
      "11/30/2018 06:31:50 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda/envs/env/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:58: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/30/2018 06:55:05 - INFO - __main__ -   *** Example ***\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   unique_id: dev-0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   tokens: [CLS] international condemnation of mug ##abe ' s win mounted yesterday with us president george bush and british foreign secretary jack straw delivering further criticism . [SEP]\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   input_ids: 101 2248 26248 1997 14757 16336 1005 1055 2663 5614 7483 2007 2149 2343 2577 5747 1998 2329 3097 3187 2990 13137 12771 2582 6256 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   label_ids: 0 0 1 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   *** Example ***\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   unique_id: dev-1\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   tokens: [CLS] bush said : ` ` we are dealing with our friends to figure out how to deal with this flawed election . ' ' straw said britain did ` ` not recognize the result or its legitimacy ' ' . [SEP]\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   input_ids: 101 5747 2056 1024 1036 1036 2057 2024 7149 2007 2256 2814 2000 3275 2041 2129 2000 3066 2007 2023 25077 2602 1012 1005 1005 13137 2056 3725 2106 1036 1036 2025 6807 1996 2765 2030 2049 22568 1005 1005 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   input_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:05 - INFO - __main__ -   label_ids: 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 2 2 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/30/2018 06:55:06 - INFO - __main__ -   ***** Running evaluation *****\n",
      "11/30/2018 06:55:06 - INFO - __main__ -     Num examples = 1218\n",
      "11/30/2018 06:55:06 - INFO - __main__ -     Batch size = 1218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'binary': {'precision': 0.8148959474260679, 'recall': 0.1624119718309859, 'f1': 0.270843722946926}, 'proportional': {'precision': 0.7605330412559325, 'recall': 0.09975792253521129, 'f1': 0.1763803164472085}}\n"
     ]
    }
   ],
   "source": [
    "class Args(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_dir = './dse/'\n",
    "        self.output_dir = '.'\n",
    "        self.bert_model = 'bert-base-uncased'\n",
    "        self.mode = 'last'\n",
    "        self.max_seq_length = 128\n",
    "        self.epochs = 200\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 1e-4\n",
    "        self.momentum = 0.7\n",
    "        self.isfrozen = True\n",
    "        self.local_rank = -1\n",
    "        self.no_cuda = False\n",
    "        self.fp16 = False\n",
    "        self.loss_scale = 128.\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        \n",
    "main(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     ## Required parameters\n",
    "#     parser.add_argument(\"--input_dir\", default=None, type=str, required=True)\n",
    "#     parser.add_argument(\"--output_dir\", default=None, type=str, required=True)\n",
    "#     parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "#                         help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "#                              \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "\n",
    "#     ## Other parameters\n",
    "#     parser.add_argument(\"--mode\", default=\"last\", type=str)\n",
    "#     parser.add_argument(\"--max_seq_length\", default=128, type=int,\n",
    "#                         help=\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"\n",
    "#                             \"than this will be truncated, and sequences shorter than this will be padded.\")\n",
    "#     parser.add_argument(\"--epochs\", default=200, type=int, help=\"Number of epoch.\")\n",
    "#     parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size for predictions.\")\n",
    "#     parser.add_argument(\"--learning_rate\", default=1e-4, type=float, help=\"Learning rate for gradient.\")\n",
    "#     parser.add_argument(\"--momentum\", default=0.7, type=float)\n",
    "#     parser.add_argument(\"--isfrozen\", default=True, type=bool)\n",
    "#     parser.add_argument(\"--local_rank\",\n",
    "#                         type=int,\n",
    "#                         default=-1,\n",
    "#                         help = \"local_rank for distributed training on gpus\")\n",
    "#     parser.add_argument(\"--no_cuda\",\n",
    "#                         default=False,\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether not to use CUDA when available\")\n",
    "#     parser.add_argument('--fp16',\n",
    "#                         default=False,\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "#     parser.add_argument('--loss_scale',\n",
    "#                         type=float, default=128,\n",
    "#                         help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "# parser.add_argument('--gradient_accumulation_steps',\n",
    "#                         type=int,\n",
    "#                         default=1,\n",
    "#                         help=\"Number of updates steps to accumualte before performing a backward/update pass.\")          \n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
