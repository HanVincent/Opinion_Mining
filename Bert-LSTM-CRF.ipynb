{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features by Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extract pre-computed feature vectors from a PyTorch BERT model.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # 神經網絡模塊中的常用功能 \n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from utils.evaluate import *\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT_PARAENTHESIS = ['-LRB-', '-LSB-', '-LCB-']\n",
    "RIGHT_PARAENTHESIS = ['-RRB-', '-RSB-', '-RCB-']\n",
    "START_TAG = \"[CLS]\"\n",
    "STOP_TAG = \"[SEP]\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, text, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "          unique_id: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.unique_id = unique_id\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens, input_ids, input_mask, input_type_ids, label_ids):\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "        self.label_ids = label_ids\n",
    "        #self.label_mask = label_mask\n",
    "        \n",
    "        \n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_label_map(self):\n",
    "        \"\"\"Gets the mapping of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_data(cls, input_file):\n",
    "        \"\"\"Reads a BIO data.\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf8') as f:\n",
    "            lines, words, labels = [], [], []\n",
    "\n",
    "            for line in f:\n",
    "                contents = line.strip()\n",
    "                word = contents.split('\\t')[0]\n",
    "                label = contents.split('\\t')[-1]\n",
    "\n",
    "\n",
    "                if len(contents) == 0: # and words[-1] == '.':\n",
    "                    w = ' '.join([word for word in words if len(word) > 0])\n",
    "                    l = ' '.join([label for label in labels if len(label) > 0])\n",
    "                    lines.append([w, l])\n",
    "                    words = []\n",
    "                    labels = []\n",
    "                    continue\n",
    "\n",
    "                if word in LEFT_PARAENTHESIS: word = '('\n",
    "                elif word in RIGHT_PARAENTHESIS: word = ')'\n",
    "                    \n",
    "                words.append(word)\n",
    "                labels.append(label)\n",
    "\n",
    "        return lines\n",
    "\n",
    "\n",
    "class NerProcessor(DataProcessor):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def get_train_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"train.txt\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"dev.txt\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self):\n",
    "        return self._create_example(\n",
    "            self._read_data(os.path.join(self.data_dir, \"test.txt\")), \"test\")\n",
    "\n",
    "    def get_label_map(self):\n",
    "        return {\"O\": 0, \"B-DSE\": 1, \"I-DSE\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "        # return {\"O\": 0, \"B-DSE\": 1, \"I-DSE\": 2, \"[CLS]\": 3, \"[SEP]\": 4}\n",
    "\n",
    "    def _create_example(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            unique_id = \"%s-%s\" % (set_type, i)\n",
    "            text, label = line\n",
    "            examples.append(InputExample(unique_id=unique_id, text=text, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    \n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "            \n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_length, tokenizer, label_map):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        ### tokenize data\n",
    "        text_list = example.text.split(' ')\n",
    "        label_list = example.label.split(' ')\n",
    "        \n",
    "        assert len(text_list) == len(label_list)\n",
    "        \n",
    "        tokens, labels = [], []\n",
    "        for i, (w, l) in enumerate(zip(text_list, label_list)):\n",
    "            tk = tokenizer.tokenize(w)\n",
    "            tokens.extend(tk)\n",
    "            for m in range(len(tk)):\n",
    "                if m == 0:         labels.append(l)\n",
    "                elif l == \"B-DSE\": labels.append(\"I-DSE\")\n",
    "                else:              labels.append(\"O\")\n",
    "\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[0 : (max_seq_length - 2)]\n",
    "            labels = labels[0 : (max_seq_length - 2)]\n",
    "\n",
    "        ### insert CLS and SEP\n",
    "        # label_ids append(\"O\") or append(\"[CLS]\") not sure!\n",
    "        ntokens, input_type_ids, label_ids = [\"[CLS]\"], [0], [label_map[\"O\"]]\n",
    "        for i, (tk, l) in enumerate(zip(tokens, labels)):\n",
    "            ntokens.append(tk)\n",
    "            input_type_ids.append(0)\n",
    "            label_ids.append(label_map[l])\n",
    "            \n",
    "        ntokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(0)\n",
    "        # append(\"O\") or append(\"[SEP]\") not sure!\n",
    "        label_ids.append(label_map[\"O\"])\n",
    "        \n",
    "        ### convert to ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
    "        \n",
    "        ### create mask\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        ### padding to max seq len\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            input_type_ids.append(0)\n",
    "            # we don't concerned about it!\n",
    "            label_ids.append(0)\n",
    "        \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(input_type_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        \n",
    "        if ex_index < 2:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"unique_id: %s\" % (example.unique_id))\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in ntokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
    "            logger.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                tokens=ntokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids,\n",
    "                label_ids=label_ids))\n",
    "        \n",
    "    features = sorted(features, key=lambda x: len(x.tokens), reverse=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_features_to_dataloader(features, local_rank, batch_size):\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.float)\n",
    "    all_input_labels = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_input_labels, all_example_index)\n",
    "    \n",
    "    if local_rank == -1: sampler = SequentialSampler(dataset)\n",
    "    else:                sampler = DistributedSampler(dataset)\n",
    "        \n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "# from pytorch_pretrained_bert.modeling import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dd040889d101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "from layers.RNN import LSTM\n",
    "from layers.layers import CRF\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "EMBED_SIZE = 300\n",
    "HIDDEN_SIZE = 1000\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "NUM_DIRS = 2 if BIDIRECTIONAL else 1\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SAVE_EVERY = 1\n",
    "\n",
    "PAD = \"<PAD>\" # padding\n",
    "SOS = \"<SOS>\" # start of sequence\n",
    "EOS = \"<EOS>\" # end of sequence\n",
    "UNK = \"<UNK>\" # unknown token\n",
    "\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = 1\n",
    "EOS_IDX = 2\n",
    "UNK_IDX = 3\n",
    "\n",
    "torch.manual_seed(1)\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "class LSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size, num_tags):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = LSTM(embedding_size, num_tags)\n",
    "        self.crf = CRF(num_tags)\n",
    "        \n",
    "    # for training\n",
    "    def forward(self, x, mask, y): \n",
    "        # mask = x.data.gt(0).float()\n",
    "        h = self.lstm(x, mask)\n",
    "        Z = self.crf.forward(h, mask)\n",
    "        score = self.crf.score(h, y, mask)\n",
    "        return Z - score # NLL loss\n",
    "\n",
    "    def decode(self, x): # for prediction\n",
    "        mask = x.data.gt(0).float()\n",
    "        h = self.lstm(x, mask)\n",
    "        return self.crf.decode(h, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "class BertTagger(nn.Module):\n",
    " \n",
    "    def __init__(self, bert_model, label_map,\n",
    "                 is_frozen=True, mode=\"last\"):\n",
    "        super(BertTagger, self).__init__()\n",
    "        \n",
    "        self.label_map = label_map\n",
    "        self.loss_function = loss_function\n",
    "        self.tagset_size = len(self.label_map)\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(bert_model)\n",
    "    \n",
    "        self.embedding_size = self.bert_model.config.hidden_size\n",
    "        self.vocab_size = self.bert_model.config.vocab_size\n",
    "\n",
    "        if is_frozen: self.bert_model.eval()\n",
    "\n",
    "        \n",
    "    def _forward_alg(self, input_ids, input_mask):\n",
    "        batch_size, max_seq_len = input_ids.shape\n",
    "        \n",
    "        all_encoder_layers, _ = self.bert_model(input_ids, \n",
    "                                                token_type_ids=None, \n",
    "                                                attention_mask=input_mask)\n",
    "        \n",
    "        if self.mode == \"last\":\n",
    "            all_encoder_layers = all_encoder_layers[-1]\n",
    "#         elif args.mode == \"weighted\":\n",
    "#             all_encoder_layers = torch.stack([a * b for a, b in zip(all_encoder_layers, self.bert_weights)])\n",
    "#             return self.bert_gamma * torch.sum(all_encoder_layers, dim=0)\n",
    "        \n",
    "        y_pred = self.lstm_crf(all_encoder_layers)\n",
    "        \n",
    "\n",
    "        ### not sure mask\n",
    "#         y_ = torch.mul(tag_scores, mask.unsqueeze(-1).expand([batch_size, seq_len, self.tagset_size]))\n",
    "        y_pred = y_pred.view(-1, self.tagset_size)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, input_labels):\n",
    "        y_pred = self._forward_alg(input_ids, input_mask)\n",
    "        y_true = input_labels.view(-1)\n",
    "\n",
    "        loss = self.lstm_crf.neg_log_likelihood(y_pred, y_true)\n",
    "\n",
    "        return y_pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.no_cuda:\n",
    "        device = torch.device(\"cpu\")\n",
    "        n_gpu = 1\n",
    "    elif args.local_rank == -1:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count() # 1\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        \n",
    "    logger.info(\"device: {} n_gpu: {} distributed training: {}\".format(device, n_gpu, bool(args.local_rank != -1)))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "\n",
    "    processor = NerProcessor(args.input_dir)\n",
    "    label_map = processor.get_label_map()\n",
    "    examples = processor.get_train_examples()\n",
    "\n",
    "    features = convert_examples_to_features(examples=examples, \n",
    "                                            max_seq_length=args.max_seq_length, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            label_map=label_map)\n",
    "    \n",
    "    dataloader = convert_features_to_dataloader(features=features, \n",
    "                                                local_rank=args.local_rank, \n",
    "                                                batch_size=args.batch_size)\n",
    "\n",
    "#     model = BertTagger(args.bert_model, label_map=label_map, mode=\"last\", \n",
    "#                        loss_function=loss_function)\n",
    "    model = BiLSTM_CRF(args.bert_model, device, mode=\"last\", \n",
    "                       hidden_dim=100, dropout=0, num_layers=1,\n",
    "                       bidirectional=True, tag_to_ix=label_map)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                              lr=args.learning_rate, momentum=args.momentum)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, \n",
    "                                                          device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    ######### TRAIN\n",
    "    best_count, tmp_loss = 0, 0\n",
    "    for epoch in range(args.epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        total_loss = 0.\n",
    "        \n",
    "        for input_ids, input_mask, input_labels, example_indices in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            input_labels = input_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(input_ids, input_mask, input_labels, is_training=True)\n",
    "    \n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args.fp16 and args.loss_scale != 1.0:\n",
    "                # rescale loss for fp16 training\n",
    "                # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "                loss = loss * args.loss_scale\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "                \n",
    "            loss = loss.mean()\n",
    "            total_loss += loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Loss:\", total_loss)\n",
    "        \n",
    "        if sum(total_loss) > tmp_loss:\n",
    "            best_count += 1\n",
    "        else:\n",
    "            tmp_loss = sum(total_loss)\n",
    "            best_count = 0\n",
    "            \n",
    "        if best_count == args.early_stop:\n",
    "            torch.save(model.state_dict(), os.path.join(args.output_dir, args.save_model_name))\n",
    "            print(\"EARLY STOPPED\")\n",
    "            break\n",
    "   \n",
    "\n",
    "    ######### TEST\n",
    "    model.load_state_dict(torch.load(os.path.join(args.output_dir, \"store.model\")))\n",
    "    \n",
    "    examples = processor.get_test_examples()\n",
    "    \n",
    "    features = convert_examples_to_features(examples=examples, \n",
    "                                             max_seq_length=args.max_seq_length, \n",
    "                                             tokenizer=tokenizer, \n",
    "                                             label_map=label_map)\n",
    "\n",
    "    dataloader = convert_features_to_dataloader(features=features, \n",
    "                                                 local_rank=args.local_rank, \n",
    "                                                 batch_size=args.batch_size)\n",
    "\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(examples))\n",
    "    logger.info(\"  Batch size = %d\", args.batch_size)\n",
    "\n",
    "    y_preds_tk_basis, y_trues_tk_basis = [], []\n",
    "    y_preds, y_trues = [], []\n",
    "    total_loss = 0.\n",
    "   \n",
    "    for input_ids, input_mask, input_labels, example_indices in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        input_labels = input_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred, loss = model(input_ids, input_mask, input_labels, is_training=False)\n",
    "            total_loss += loss\n",
    "\n",
    "#             y_pred = torch.max(y_pred, 2)[1].view([len(input_ids), -1])\n",
    "            y_pred = torch.max(y_pred, 1)[1].view([len(input_ids), -1])\n",
    "\n",
    "            ### 用 * mask \n",
    "            # y_pred = torch.mul(y_pred, input_mask) # [batch, max_seq_len]\n",
    "            # y_true = torch.mul(input_labels, input_mask)\n",
    "            y_pred = [y_[ : sum(input_mask[i])] for i, y_ in enumerate(y_pred)]\n",
    "            y_true = [y[ : sum(input_mask[i])] for i, y in enumerate(input_labels)]\n",
    "          \n",
    "            y_preds.extend(y_pred)\n",
    "            y_trues.extend(y_true)\n",
    "            y_preds_tk_basis.extend([el for y_ in y_pred for el in y_])\n",
    "            y_trues_tk_basis.extend([el for y_ in y_true for el in y_])\n",
    "            \n",
    "\n",
    "    print(\"loss:\", total_loss)\n",
    "    \n",
    "    result = token_basis_evaluate(y_preds_tk_basis, y_trues_tk_basis, label_map, output_dict=False)\n",
    "    print(result)\n",
    "\n",
    "    result = overlap_evaluate(y_preds, y_trues, label_map)\n",
    "    print(result)    \n",
    "\n",
    "    import pickle\n",
    "    with open(os.path.join(args.output_dir, \"results.txt\"), \"wb\") as file:\n",
    "        pickle.dump([y_preds, y_trues, y_preds_tk_basis, y_trues_tk_basis], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_dir = './dse/'\n",
    "        self.output_dir = '.'\n",
    "        self.save_model_name = 'bert_bilstm_crf.model'\n",
    "        self.bert_model = 'bert-base-uncased'\n",
    "        self.mode = 'last'\n",
    "        self.max_seq_length = 128\n",
    "        self.epochs = 5\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 3e-1\n",
    "        self.momentum = 0.7\n",
    "        self.is_frozen = True\n",
    "        self.local_rank = -1\n",
    "        self.no_cuda = False\n",
    "        self.fp16 = False\n",
    "        self.loss_scale = 128.\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.early_stop = 5\n",
    "        \n",
    "main(Args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     ## Required parameters\n",
    "#     parser.add_argument(\"--input_dir\", default=None, type=str, required=True)\n",
    "#     parser.add_argument(\"--output_dir\", default=None, type=str, required=True)\n",
    "#     parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "#                         help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "#                              \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
    "\n",
    "#     ## Other parameters\n",
    "#     parser.add_argument(\"--mode\", default=\"last\", type=str)\n",
    "#     parser.add_argument(\"--max_seq_length\", default=128, type=int,\n",
    "#                         help=\"The maximum total input sequence length after WordPiece tokenization. Sequences longer \"\n",
    "#                             \"than this will be truncated, and sequences shorter than this will be padded.\")\n",
    "#     parser.add_argument(\"--epochs\", default=200, type=int, help=\"Number of epoch.\")\n",
    "#     parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size for predictions.\")\n",
    "#     parser.add_argument(\"--learning_rate\", default=1e-4, type=float, help=\"Learning rate for gradient.\")\n",
    "#     parser.add_argument(\"--momentum\", default=0.7, type=float)\n",
    "#     parser.add_argument(\"--is_frozen\", default=True, type=bool)\n",
    "#     parser.add_argument(\"--local_rank\",\n",
    "#                         type=int,\n",
    "#                         default=-1,\n",
    "#                         help = \"local_rank for distributed training on gpus\")\n",
    "#     parser.add_argument(\"--no_cuda\",\n",
    "#                         default=False,\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether not to use CUDA when available\")\n",
    "#     parser.add_argument('--fp16',\n",
    "#                         default=False,\n",
    "#                         action='store_true',\n",
    "#                         help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "#     parser.add_argument('--loss_scale',\n",
    "#                         type=float, default=128,\n",
    "#                         help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "# parser.add_argument('--gradient_accumulation_steps',\n",
    "#                         type=int,\n",
    "#                         default=1,\n",
    "#                         help=\"Number of updates steps to accumualte before performing a backward/update pass.\")          \n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
