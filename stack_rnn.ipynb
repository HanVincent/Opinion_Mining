{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch\n",
    "import torch.autograd as autograd # torch中自動計算梯度模塊\n",
    "import torch.nn as nn             # 神經網絡模塊\n",
    "import torch.nn.functional as F   # 神經網絡模塊中的常用功能 \n",
    "import torch.optim as optim       # 模型優化器模塊\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"LANG\"] = \"en_US.UTF-8\"\n",
    "os.environ[\"LC_CTYPE\"] = \"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_pairs(file, ratio=0.1):\n",
    "    pairs = []\n",
    "    entries = open(file, 'r', encoding='utf8').read().strip().split('\\n\\n')\n",
    "    num = int(len(entries) * ratio)\n",
    "    \n",
    "    for entry in entries:\n",
    "        sentence, target = [], []\n",
    "        for line in entry.split('\\n'):\n",
    "            if line.strip() == '': continue\n",
    "                \n",
    "            token, pos, bio = line.split('\\t')\n",
    "            if token == '-LRB-': token = '('\n",
    "            elif token == '-RRB-': token = ')'\n",
    "                \n",
    "            sentence.append(token)\n",
    "            target.append(bio)\n",
    "        pairs.append((sentence, target))\n",
    "\n",
    "    return pairs[num:], pairs[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13043 13043\n"
     ]
    }
   ],
   "source": [
    "dse_train, dse_test = get_data_pairs('./dataset/dse.txt')\n",
    "ese_train, ese_test = get_data_pairs('./dataset/ese.txt')\n",
    "dse_train.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "ese_train.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "dse_test.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "ese_test.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "print(len(dse_train), len(ese_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_ixs(seq, to_ix):\n",
    "    ixs = [to_ix[w] if w in to_ix else to_ix[UNK_TOKEN] for w in seq]\n",
    "    return torch.cuda.LongTensor(ixs)\n",
    "#     return ixs\n",
    "\n",
    "\n",
    "def ixs_to_sequence(seq, to_word):\n",
    "    tokens = [to_word[ix] for ix in seq]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def padding(seq, max_size):\n",
    "    diff = max_size - len(seq)\n",
    "    return seq + [PAD_TOKEN] * diff\n",
    "\n",
    "\n",
    "def batch_padding(seqs, max_size):\n",
    "    return [ padding(seq, max_size) for seq in seqs]\n",
    "\n",
    "\n",
    "def batch_seq_to_idx(seqs, to_ix):\n",
    "    return [ sequence_to_ixs(seq, to_ix) for seq in seqs]\n",
    "\n",
    "\n",
    "# for gensim word2vec\n",
    "# def sequence_to_ixs2(seq):\n",
    "#     vocabs = word_vectors.vocab.keys()\n",
    "#     ixs = [word_vectors.vocab[w].index if w in vocabs else 0 for w in seq]\n",
    "#     tensor = torch.cuda.LongTensor(ixs)\n",
    "    \n",
    "#     return autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/scepter/word_vectors/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "# word_vectors.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    " \n",
    "    def __init__(self, embedding_dim, hidden_dim, \n",
    "                 vocab_size, tagset_size, \n",
    "                 dropout, num_layers, bidirectional):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.direction = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim // self.direction\n",
    "        self.num_layers = num_layers\n",
    "        self.tagset_size = tagset_size\n",
    "\n",
    "        # self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        weights = torch.cuda.FloatTensor(embedding_weights)\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, \n",
    "                            dropout=dropout, num_layers=self.num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    " \n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim * self.direction, self.tagset_size)\n",
    "    \n",
    "        # self.hidden = self.init_hidden()\n",
    " \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (autograd.Variable(torch.randn(self.num_layers * self.direction, batch_size, self.hidden_dim).cuda()),\n",
    "                autograd.Variable(torch.randn(self.num_layers * self.direction, batch_size, self.hidden_dim).cuda()))\n",
    " \n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        batch_size, seq_len = sentence.shape\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        try:\n",
    "            embeds = self.word_embeddings(sentence) # [batch_size, seq_len, emb_dim]\n",
    "            # embeds = embeds.view(seq_len, batch_size, -1)\n",
    "            embeds = pack_padded_sequence(embeds, lengths, batch_first=True)\n",
    "            \n",
    "            lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "            lstm_out, lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "            tag_space = self.hidden2tag(lstm_out.contiguous().view(batch_size * seq_len, -1))\n",
    "            tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "#             lstm_out = lstm_out.contiguous()\n",
    "#             tag_space = self.hidden2tag(lstm_out.view(batch_size, -1, lstm_out.shape[2]))\n",
    "#             tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "#             tag_scores = tag_scores.view(batch_size, -1, self.tagset_size)\n",
    "\n",
    "            return tag_scores\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(sentence.shape)\n",
    "            print(embeds.shape)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(file, maximum=50000000000):\n",
    "    glove, embedding_weights = {}, []\n",
    "    ix, word_to_ix, ix_to_word = 0, {}, {}\n",
    "\n",
    "    for line in open(file, 'r', encoding='utf8').readlines():\n",
    "        line = line.strip().split(' ')\n",
    "        if len(line) != (embedding_dim + 1): continue\n",
    "        if line[0] in glove: continue\n",
    "\n",
    "        vec = np.array(line[1:]).astype(np.float32)\n",
    "        glove[line[0]] = vec\n",
    "        embedding_weights.append(vec)\n",
    "            \n",
    "        word_to_ix[line[0]] = ix\n",
    "        ix_to_word[ix] = line[0]\n",
    "        ix += 1\n",
    "        \n",
    "        if ix > maximum: break\n",
    "\n",
    "    glove[UNK_TOKEN] = [0] * len(embedding_weights[0])\n",
    "    embedding_weights.append([0] * len(embedding_weights[0]))\n",
    "    word_to_ix[UNK_TOKEN] = ix\n",
    "    ix_to_word[ix] = UNK_TOKEN\n",
    "    \n",
    "    ix += 1\n",
    "    glove[PAD_TOKEN] = [0] * len(embedding_weights[0])\n",
    "    embedding_weights.append([0] * len(embedding_weights[0]))\n",
    "    word_to_ix[PAD_TOKEN] = ix\n",
    "    ix_to_word[ix] = PAD_TOKEN\n",
    "    \n",
    "    assert len(glove) == len(embedding_weights)\n",
    "    \n",
    "    print(len(glove))\n",
    "    \n",
    "    return glove, embedding_weights, word_to_ix, ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = '<UNK>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "\n",
    "embedding_dim = 300\n",
    "hidden_dim = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.7\n",
    "dropout = 0\n",
    "num_layers = 3\n",
    "bidirectional = True\n",
    "batch_size = 1\n",
    "epochs = 200\n",
    "vector_file= 'dataset/glove/glove.840B.300d.txt'\n",
    "model_path = 'models/standard.model'\n",
    "\n",
    "train_data = dse_train\n",
    "test_data = dse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove, embedding_weights, word_to_ix, ix_to_word = get_vectors(vector_file)\n",
    "# embedding_weights = np.array(embedding_weights).astype(np.float32)\n",
    "\n",
    "# with open('dataset/glove.pickle', 'wb') as handle:\n",
    "#     pickle.dump( [glove, embedding_weights, word_to_ix, ix_to_word] , handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('dataset/glove.pickle', 'rb') as handle:\n",
    "    glove, embedding_weights, word_to_ix, ix_to_word = pickle.load(handle)\n",
    "\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, PAD_TOKEN: 3} # 手工設定詞性標籤數據字典\n",
    "ix_to_tag = {0: \"B\", 1: \"I\", 2: \"O\", 3: PAD_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(embedding_dim, hidden_dim, \n",
    "                   len(word_to_ix), len(tag_to_ix), \n",
    "                   dropout=dropout,\n",
    "                   num_layers=num_layers,\n",
    "                   bidirectional=bidirectional)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.40723931789398193\n",
      "epoch: 10, loss: 0.4389103949069977\n",
      "epoch: 15, loss: 0.2720201313495636\n",
      "epoch: 20, loss: 0.25212177634239197\n",
      "epoch: 25, loss: 0.2503272294998169\n",
      "epoch: 30, loss: 0.216319277882576\n",
      "epoch: 35, loss: 0.1860293596982956\n",
      "epoch: 40, loss: 0.3812653720378876\n",
      "epoch: 45, loss: 0.25943294167518616\n",
      "epoch: 50, loss: 0.12054189294576645\n",
      "epoch: 55, loss: 0.41587457060813904\n",
      "epoch: 60, loss: 0.2960790693759918\n",
      "epoch: 65, loss: 0.33402273058891296\n",
      "epoch: 70, loss: 0.33780160546302795\n",
      "epoch: 75, loss: 0.15051205456256866\n",
      "epoch: 80, loss: 0.28262749314308167\n",
      "epoch: 85, loss: 0.19751279056072235\n",
      "epoch: 90, loss: 0.17423240840435028\n",
      "epoch: 95, loss: 0.2889549732208252\n",
      "epoch: 100, loss: 0.1813994199037552\n",
      "epoch: 105, loss: 0.19740454852581024\n",
      "epoch: 110, loss: 0.11763087660074234\n",
      "epoch: 115, loss: 0.07929380983114243\n",
      "epoch: 120, loss: 0.11340045928955078\n",
      "epoch: 125, loss: 0.09135881811380386\n",
      "epoch: 130, loss: 0.21840937435626984\n",
      "epoch: 135, loss: 0.2819167375564575\n",
      "epoch: 140, loss: 0.1737666130065918\n",
      "epoch: 145, loss: 0.46052685379981995\n",
      "epoch: 150, loss: 0.10350457578897476\n",
      "epoch: 155, loss: 0.20741526782512665\n",
      "epoch: 160, loss: 0.11883004754781723\n",
      "epoch: 165, loss: 0.098399318754673\n",
      "epoch: 170, loss: 0.1547560691833496\n",
      "epoch: 175, loss: 0.1418665200471878\n",
      "epoch: 180, loss: 0.08929149061441422\n",
      "epoch: 185, loss: 0.7686159610748291\n",
      "epoch: 190, loss: 0.22222892940044403\n",
      "epoch: 195, loss: 0.15557026863098145\n",
      "epoch: 200, loss: 0.09432324022054672\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "\n",
    "total_num = len(train_data)\n",
    "batch_num = math.ceil(total_num / batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # for sentence, tags in train_data: \n",
    "    for i in range(batch_num):\n",
    "        model.zero_grad()\n",
    "        # model.hidden = model.init_hidden()\n",
    "        \n",
    "        data = train_data[i * batch_size : (i+1) * batch_size]\n",
    "        \n",
    "        # iterate: padding -> to_ix -> list\n",
    "        # max_size = len(data[-1][0]) # get last one size in each batch\n",
    "        # x = list(map(lambda x: sequence_to_ixs(padding(x[0], max_size), word_to_ix), data))\n",
    "        # y = list(map(lambda x: sequence_to_ixs(padding(x[1], max_size), tag_to_ix), data))\n",
    "        \n",
    "        x = list(map(lambda pair: sequence_to_ixs(pair[0], word_to_ix), data))\n",
    "        y = list(map(lambda pair: sequence_to_ixs(pair[1], tag_to_ix), data))\n",
    "        \n",
    "        assert len(x) == len(y)\n",
    "        \n",
    "        # x = autograd.Variable(torch.cuda.LongTensor(x))\n",
    "        # y = autograd.Variable(torch.cuda.LongTensor(y))\n",
    "\n",
    "        lengths = list(map(lambda x: x.shape[0], x))\n",
    "        \n",
    "        padded_seqs = pad_sequence(x, batch_first=True)\n",
    "        padded_tags = pad_sequence(y, batch_first=True)\n",
    "        \n",
    "        predict_tags = model(padded_seqs, lengths)\n",
    "        true_tags = padded_tags.view(-1)\n",
    "        \n",
    "        # one vs one\n",
    "        # sentence_in = sequence_to_ixs(sentence, word_to_ix)\n",
    "        # targets = sequence_to_ixs(tags, tag_to_ix)\n",
    "        # tag_scores = model(sentence_in)\n",
    "\n",
    "        loss = loss_function(predict_tags, true_tags)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"epoch: {}, loss: {}\".format(epoch+1, loss))\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(tag_seq):\n",
    "    segs = []\n",
    "    start = -1\n",
    "    for i, y in enumerate(tag_seq):\n",
    "        if y == tag_to_ix[\"O\"]: \n",
    "            if start != -1: segs.append((start, i))\n",
    "            start = -1\n",
    "        elif y == tag_to_ix[\"B\"]:\n",
    "            if start != -1: segs.append((start, i))\n",
    "            start = i\n",
    "        elif y == tag_to_ix[\"I\"]:\n",
    "            if start == -1: start = i\n",
    "        else:\n",
    "            print(y)\n",
    "    \n",
    "    if start != -1 and start != len(tag_seq):\n",
    "        segs.append((start, len(tag_seq)))\n",
    "        \n",
    "    return segs\n",
    "\n",
    "\n",
    "def show(y_predict, y_true):\n",
    "    ps = [ix_to_tag[ix] for ix in y_predict.cpu().numpy()]\n",
    "    ts = [ix_to_tag[ix] for ix in y_true.cpu().numpy()]\n",
    "    \n",
    "    print(\"Predict: {}\\tTrue: {}\".format(''.join(ps), ''.join(ts)))\n",
    "\n",
    "\n",
    "def evaluate(predicts, trues):\n",
    "    assert len(predicts) == len(trues)\n",
    "    \n",
    "    precision_prop, recall_prop = .0, .0\n",
    "    precision_bin, recall_bin = 0, 0\n",
    "    predict_total, true_total = 0, 0\n",
    "    \n",
    "    for y_predict, y_true in zip(predicts, trues):\n",
    "        assert len(y_predict) == len(y_true)\n",
    "\n",
    "        predict_segs = get_segments(y_predict)\n",
    "        true_segs = get_segments(y_true)\n",
    "\n",
    "        predict_count = len(predict_segs)\n",
    "        true_count = len(true_segs)\n",
    "        \n",
    "        predict_total += predict_count\n",
    "        true_total += true_count\n",
    "        \n",
    "        predict_flags = [False for i in range(predict_count)]\n",
    "        true_flags = [False for i in range(true_count)]\n",
    "\n",
    "        for t_i, (t_start, t_end) in enumerate(true_segs):\n",
    "            for p_i, (p_start, p_end) in enumerate(predict_segs):\n",
    "                assert p_start != p_end\n",
    "\n",
    "                l_max = t_start if t_start > p_start else p_start\n",
    "                r_min = t_end   if t_end   < p_end else p_end\n",
    "                overlap = (r_min - l_max) if r_min > l_max else 0\n",
    "                \n",
    "                precision_prop += overlap / (p_end - p_start)\n",
    "                recall_prop += overlap / (t_end - t_start)\n",
    "\n",
    "                if not predict_flags[p_i] and overlap > 0:\n",
    "                    precision_bin += 1\n",
    "                    predict_flags[p_i] = True\n",
    "                if not true_flags[t_i] and overlap > 0:\n",
    "                    recall_bin += 1\n",
    "                    true_flags[t_i] = True\n",
    "\n",
    "                    \n",
    "        # show(y_predict, y_true)\n",
    "        \n",
    "    precision = (precision_bin / predict_total) if predict_total != 0 else 1\n",
    "    recall = recall_bin / true_total\n",
    "    f1 = (2 * precision * recall) / (precision + recall)    \n",
    "    binary_overlap = { 'precision': precision, 'recall': recall, 'f1': f1 }\n",
    "    \n",
    "    precision = (precision_prop / predict_total) if predict_total != 0 else 1\n",
    "    recall = recall_prop / true_total\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    proportional_overlap = { 'precision': precision, 'recall': recall, 'f1': f1 }\n",
    "    \n",
    "    print(\"Test data length: {}\".format(len(predicts)))\n",
    "    print(\"Precision\\tBin: {}, Prop: {:.2f}, Predict Total: {}\".format(precision_bin, precision_prop, predict_total))\n",
    "    print(\"Recall\\t\\tBin: {}, Prop: {:.2f}, Recall Total: {}\".format(recall_bin, recall_prop, true_total))\n",
    "    print(\"=\" * 75)\n",
    "    print(\"Binary Overlap\\t\\tPrecision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\".format(**binary_overlap))\n",
    "    print(\"Proportional Overlap\\tPrecision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\".format(**proportional_overlap))\n",
    "    \n",
    "    return { 'binary': binary_overlap, 'proportional': proportional_overlap }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 1449\n",
      "Precision\tBin: 819, Prop: 769.50, Predict Total: 1086\n",
      "Recall\t\tBin: 798, Prop: 697.05, Recall Total: 1333\n",
      "===========================================================================\n",
      "Binary Overlap\t\tPrecision: 0.75, Recall: 0.60, F1: 0.67\n",
      "Proportional Overlap\tPrecision: 0.71, Recall: 0.52, F1: 0.60\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "with torch.no_grad():\n",
    "    data = test_data\n",
    "    \n",
    "    x = list(map(lambda pair: sequence_to_ixs(pair[0], word_to_ix), data))\n",
    "    y = list(map(lambda pair: sequence_to_ixs(pair[1], tag_to_ix), data))\n",
    "\n",
    "    lengths = list(map(lambda x: x.shape[0], x))\n",
    "\n",
    "    padded_seqs = pad_sequence(x, batch_first=True)\n",
    "    y_predicts = model(padded_seqs, lengths)\n",
    "    y_predicts = torch.max(y_predicts, 1)[1].view([len(lengths), -1]) # .cpu().numpy()\n",
    "    \n",
    "    y_trues = y\n",
    "    y_predicts = [y_[:lengths[i]] for i, y_ in enumerate(y_predicts)]\n",
    "\n",
    "#     y_predicts, y_trues = [], []\n",
    "#     for i in range(batch_num):\n",
    "#         data = test_data[i * batch_size : (i+1) * batch_size]\n",
    "            \n",
    "#         seq, true_targets = each\n",
    "#         inputs = autograd.Variable(torch.cuda.LongTensor(sequence_to_ixs(seq, word_to_ix)))\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         predict_targets = model(inputs)        \n",
    "#         predict_targets = torch.max(predict_targets, 1)[1].cpu().numpy()\n",
    "#         predict_targets = ixs_to_sequence(predict_targets, ix_to_tag)\n",
    "\n",
    "#         y_predicts.append(predict_targets)\n",
    "#         y_trues.append(true_targets)\n",
    "\n",
    "    # 感覺可以實驗 tag by tag\n",
    "    evaluate(y_predicts, y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
